{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation in Python\n",
    "\n",
    "### Assign daily acquisition cohort\n",
    "As you have seen in the video, defining a cohort is the first step to cohort analysis. You will now create daily cohorts based on the day each customer has made their first transaction.\n",
    "\n",
    "The data has been loaded as online DataFrame, you can now print its header with online.head() in the console.\n",
    "```python\n",
    "# Define a function that will parse the date\n",
    "def get_day(x): return dt.datetime(x.year, x.month, x.day) \n",
    "\n",
    "# Create InvoiceDay column\n",
    "online['InvoiceDay'] = online['InvoiceDate'].apply(get_day) \n",
    "\n",
    "# Group by CustomerID and select the InvoiceDay value\n",
    "grouping = online.groupby('CustomerID')['InvoiceDate'] \n",
    "\n",
    "# Assign a minimum InvoiceDay value to the dataset\n",
    "online['CohortDay'] = grouping.transform('min')\n",
    "\n",
    "# View the top 5 rows\n",
    "print(online.head())\n",
    "\n",
    "```\n",
    "Calculate time offset in days - part 1\n",
    "Calculating time offset for each transaction allows you to report the metrics for each cohort in a comparable fashion.\n",
    "\n",
    "First, we will create 6 variables that capture the integer value of years, months and days for Invoice and Cohort Date using the get_date_int() function that's been already defined for you:\n",
    "```python\n",
    "def get_date_int(df, column):\n",
    "    year = df[column].dt.year\n",
    "    month = df[column].dt.month\n",
    "    day = df[column].dt.day\n",
    "    return year, month, day\n",
    "```\n",
    "The online data has been loaded, you can print its header to the console by calling online.head().\n",
    "\n",
    "```python\n",
    "# Get the integers for date parts from the `InvoiceDay` column\n",
    "invoice_year, invoice_month, invoice_day = get_date_int(online, 'InvoiceDay')\n",
    "\n",
    "# Get the integers for date parts from the `CohortDay` column\n",
    "cohort_year, cohort_month, cohort_day = get_date_int(online, 'CohortDay')\n",
    "\n",
    "```\n",
    "### Calculate time offset in days - part 2\n",
    "Great work! Now, we have six different data sets with year, month and day values for Invoice and Cohort dates - invoice_year, cohort_year, invoice_month, cohort_month, invoice_day, and cohort_day.\n",
    "\n",
    "In this exercise you will calculate the difference between the Invoice and Cohort dates in years, months and days separately and then calculate the total days difference between the two. This will be your days offset which we will use in the next exercise to visualize the customer count. The online data has been loaded, you can print its header to the console by calling online.head().\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Calculate difference in years\n",
    "years_diff = invoice_year - cohort_year\n",
    "\n",
    "# Calculate difference in months\n",
    "months_diff = invoice_month - cohort_month\n",
    "\n",
    "# Calculate difference in days\n",
    "days_diff = invoice_day - cohort_day\n",
    "\n",
    "# Extract the difference in days from all previous values\n",
    "online['CohortIndex'] = years_diff * 365 + months_diff * 30 + days_diff + 1\n",
    "print(online.head())\n",
    "\n",
    "```\n",
    "### Calculate retention rate from scratch\n",
    "You have seen how to create retention and average quantity metrics table for the monthly acquisition cohorts. Now it's you time to build the retention metrics by yourself.\n",
    "\n",
    "The online dataset has been loaded to you with monthly cohorts and cohort index assigned from this lesson. Feel free to print it in the Console.\n",
    "\n",
    "Also, we have created a loaded a groupby object as grouping DataFrame with this command: grouping = online.groupby(['CohortMonth', 'CohortIndex'])\n",
    "\n",
    "```python\n",
    "# Count the number of unique values per customer ID\n",
    "cohort_data = grouping['CustomerID'].apply(pd.Series.nunique).reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "cohort_counts = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='CustomerID')\n",
    "\n",
    "# Select the first column and store it to cohort_sizes\n",
    "cohort_sizes = cohort_counts.iloc[:,0]\n",
    "\n",
    "# Divide the cohort count by cohort sizes along the rows\n",
    "retention = cohort_counts.divide(cohort_sizes, axis=0)\n",
    "\n",
    "```\n",
    "### Calculate average price\n",
    "You will now calculate the average price metric and analyze if there are any differences in shopping patterns across time and across cohorts.\n",
    "\n",
    "The online dataset has been loaded to you with monthly cohorts and cohort index assigned from this lesson. Feel free to print it to the Console\n",
    "\n",
    "```python\n",
    "# Create a groupby object and pass the monthly cohort and cohort index as a list\n",
    "grouping = online.groupby(['CohortMonth', 'CohortIndex'])\n",
    "\n",
    "# Calculate the average of the unit price column\n",
    "cohort_data = grouping['UnitPrice'].mean()\n",
    "\n",
    "# Reset the index of cohort_data\n",
    "cohort_data = cohort_data.reset_index()\n",
    "\n",
    "# Create a pivot \n",
    "average_price = cohort_data.pivot(index='CohortMonth', columns='CohortIndex', values='UnitPrice')\n",
    "print(average_price.round(1))\n",
    "\n",
    "```\n",
    "### Visualize average quantity metric\n",
    "You are now going to visualize average quantity values in a heatmap.\n",
    "\n",
    "We have loaded pandas package as pd, and the average quantity values DataFrame as average_quantity.\n",
    "\n",
    "Please use the console to explore it.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "# Import seaborn package as sns\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize an 8 by 6 inches plot figure\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# Add a title\n",
    "plt.title('Average Spend by Monthly Cohorts')\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(data=average_quantity, annot=True, cmap='Blues')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "### Calculate Spend quartiles (q=4)\n",
    "We have created a dataset for you with random CustomerID and Spend values as data. You will now use this dataset to group customers into quartiles based on Spend values and assign labels to each of them.\n",
    "\n",
    "pandas library as been loaded as pd. Feel free to print the data to the console.\n",
    "\n",
    "\n",
    "```python\n",
    "CustomerID  Spend\n",
    "0           0    137\n",
    "1           1    335\n",
    "2           2    172\n",
    "3           3    355\n",
    "4           4    303\n",
    "5           5    233\n",
    "6           6    244\n",
    "7           7    229\n",
    "\n",
    "```\n",
    "```python\n",
    "# Create a spend quartile with 4 groups - a range between 1 and 5\n",
    "spend_quartile = pd.qcut(data['Spend'], q=4, labels=range(1,5))\n",
    "\n",
    "# Assign the quartile values to the Spend_Quartile column in data\n",
    "data['Spend_Quartile'] = spend_quartile\n",
    "\n",
    "# Print data with sorted Spend values\n",
    "print(data.sort_values('Spend'))\n",
    "\n",
    "     CustomerID  Spend Spend_Quartile\n",
    "    0           0    137              1\n",
    "    2           2    172              1\n",
    "    7           7    229              2\n",
    "    5           5    233              2\n",
    "    6           6    244              3\n",
    "    4           4    303              3\n",
    "    1           1    335              4\n",
    "    3           3    355              4\n",
    "\n",
    "```\n",
    "### Calculate Recency deciles (q=10)\n",
    "We have created a dataset for you with random CustomerID and Recency_Days values as data. You will now use this dataset to group customers into quartiles based on Recency_Days values and assign labels to each of them.\n",
    "\n",
    "Be cautious about the labels for this exercise. You will see that the labels are inverse, and will required one additional step in separately creating them. If you need to refresh your memory on the process of creating the labels, check out the slides!\n",
    "\n",
    "The pandas library as been loaded as pd. Feel free to print the data to the console.\n",
    "```python\n",
    "# Store labels from 4 to 1 in a decreasing order\n",
    "r_labels = list(range(4, 0, -1))\n",
    "\n",
    "# Create a spend quartile with 4 groups and pass the previously created labels \n",
    "recency_quartiles = pd.qcut(data['Recency_Days'], q=4, labels=r_labels)\n",
    "\n",
    "# Assign the quartile values to the Recency_Quartile column in `data`\n",
    "data['Recency_Quartile'] = recency_quartiles \n",
    "\n",
    "# Print `data` with sorted Recency_Days values\n",
    "print(data.sort_values('Recency_Days'))\n",
    "\n",
    "    CustomerID  Recency_Days Recency_Quartile\n",
    "    0           0            37                4\n",
    "    3           3            72                4\n",
    "    7           7           133                3\n",
    "    6           6           203                3\n",
    "    1           1           235                2\n",
    "    4           4           255                2\n",
    "    5           5           393                1\n",
    "    2           2           396                1\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "### Calculate RFM values\n",
    "Calculate Recency, Frequency and Monetary values for the online dataset we have used before - it has been loaded for you with recent 12 months of data. There's a TotalSum column in the online dataset which has been calculated by multiplying Quantity and UnitPrice: online['Quantity'] * online['UnitPrice'].\n",
    "\n",
    "Also, we have created a snapshot_date variable that you can use to calculate recency. Feel free to print the online dataset and the snapshot_date into the Console. The pandas library is loaded as pd, and datetime as dt.\n",
    "```python\n",
    "# Calculate Recency, Frequency and Monetary value for each customer \n",
    "datamart = online.groupby(['CustomerID']).agg({\n",
    "    'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "# Rename the columns \n",
    "datamart.rename(columns={'InvoiceDate': 'Recency',\n",
    "                         'InvoiceNo': 'Frequency',\n",
    "                         'TotalSum': 'MonetaryValue'}, inplace=True)\n",
    "\n",
    "# Print top 5 rows\n",
    "print(datamart.head())\n",
    "\n",
    "            Recency  Frequency  MonetaryValue\n",
    "    CustomerID                                   \n",
    "    12747             3         25         948.70\n",
    "    12748             1        888        7046.16\n",
    "    12749             4         37         813.45\n",
    "    12820             4         17         268.02\n",
    "    12822            71          9         146.15\n",
    "\n",
    "\n",
    "```\n",
    "### Calculate 3 groups for Recency and Frequency\n",
    "You will now group the customers into three separate groups based on Recency, and Frequency.\n",
    "\n",
    "The dataset has been loaded as datamart, you can use console to view top rows of it. Also, pandas has been loaded as pd.\n",
    "\n",
    "We will use the result from the exercise in the next one, where you will group customers based on the MonetaryValue and finally calculate and RFM_Score.\n",
    "\n",
    "Once completed, print the results to the screen to make sure you have successfully created the quartile columns.\n",
    "```python\n",
    "# Create labels for Recency and Frequency\n",
    "r_labels = range(3, 0, -1); f_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "r_groups = pd.qcut(datamart['Recency'], q=3, labels=r_labels)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "f_groups = pd.qcut(datamart['Frequency'], q=3, labels=f_labels)\n",
    "\n",
    "# Create new columns R and F \n",
    "datamart = datamart.assign(R=r_groups.values, F=f_groups.values)\n",
    "print(datamart.head())\n",
    "\n",
    "```\n",
    "Calculate RFM Score\n",
    "Great work, you will now finish the job by assigning customers to three groups based on the MonetaryValue percentiles and then calculate an RFM_Score which is a sum of the R, F, and M values.\n",
    "\n",
    "The datamart has been loaded with the R and F values you have created in the previous exercise.\n",
    "```python\n",
    "# Create labels for MonetaryValue\n",
    "m_labels = range(1, 4)\n",
    "\n",
    "# Assign these labels to three equal percentile groups \n",
    "m_groups = pd.qcut(datamart['MonetaryValue'], q=3, labels=m_labels)\n",
    "\n",
    "# Create new column M\n",
    "datamart = datamart.assign(M=m_groups.values)\n",
    "\n",
    "# Calculate RFM_Score\n",
    "datamart['RFM_Score'] = datamart[['R','F','M']].sum(axis=1)\n",
    "print(datamart['RFM_Score'].head())\n",
    "\n",
    "```\n",
    "### Creating custom segments\n",
    "It's your turn to create a custom segmentation based on RFM_Score values. You will create a function to build segmentation and then assign it to each customer.\n",
    "\n",
    "The dataset with the RFM values, RFM Segment and Score has been loaded as datamart, together with pandas and numpy libraries. Feel free to explore the data in the console.\n",
    "```python\n",
    "# Define rfm_level function\n",
    "def rfm_level(df):\n",
    "    if df['RFM_Score'] >= 10:\n",
    "        return 'Top'\n",
    "    elif ((df['RFM_Score'] >= 6) and (df['RFM_Score'] < 10)):\n",
    "        return 'Middle'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "# Create a new variable RFM_Level\n",
    "datamart['RFM_Level'] = datamart.apply(rfm_level, axis=1)\n",
    "\n",
    "# Print the header with top 5 rows to the console\n",
    "print(datamart.head())\n",
    "\n",
    "```\n",
    "### Analyzing custom segments\n",
    "As a final step, you will analyze average values of Recency, Frequency and MonetaryValue for the custom segments you've created.\n",
    "\n",
    "We have loaded the datamart dataset with the segment values you have calculated in the previous exercise. Feel free to explore it in the console. pandas library is also loaded as pd.\n",
    "```python\n",
    "# Calculate average values for each RFM_Level, and return a size of each segment \n",
    "rfm_level_agg = datamart.groupby('RFM_Level').agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "  \n",
    "  \t# Return the size of each segment\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "}).round(1)\n",
    "\n",
    "# Print the aggregated dataset\n",
    "print(rfm_level_agg)\n",
    "\n",
    "RFM_Level                                      \n",
    "Low         180.8       3.2          52.7  1075\n",
    "Middle       73.9      10.7         202.9  1547\n",
    "Top          20.3      47.1         959.7  1021\n",
    "\n",
    "```\n",
    "### Calculate statistics of variables\n",
    "We have created a pandas DataFrame called data for you with three variables: var1, var2 and var3.\n",
    "\n",
    "You will now calculate average and standard deviation values for the variables, and also print key statistics of the dataset.\n",
    "\n",
    "You can use the console to explore the dataset.\n",
    "```python\n",
    "# Print the average values of the variables in the dataset\n",
    "print(data.mean())\n",
    "\n",
    "# Print the standard deviation of the variables in the dataset\n",
    "print(data.std())\n",
    "\n",
    "# Get the key statistics of the dataset\n",
    "print(data.describe())\n",
    "\n",
    "var1    251.85000\n",
    "var2      1.92559\n",
    "var3     12.55028\n",
    "dtype: float64\n",
    "var1    90.993104\n",
    "var2     2.583730\n",
    "var3    34.516362\n",
    "dtype: float64\n",
    "             var1       var2        var3\n",
    "count  100.000000  100.00000  100.000000\n",
    "mean   251.850000    1.92559   12.550280\n",
    "std     90.993104    2.58373   34.516362\n",
    "min    101.000000    0.04800    0.002000\n",
    "25%    171.750000    0.61250    0.280750\n",
    "50%    250.000000    1.17550    1.260500\n",
    "75%    339.250000    2.20800    5.568000\n",
    "max    397.000000   15.31200  228.779000\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "### Detect skewed variables\n",
    "We have created a dataset called data for you with three variables: var1, var2 and var3. You will now explore their distributions.\n",
    "\n",
    "The plt.subplot(...) call before the seaborn function call allows you to plot several subplots in one chart, you do not have to change it.\n",
    "\n",
    "Libraries seaborn and matplotlib.pyplot have been loaded as sns and plt respectively. Feel free to explore the dataset in the console.\n",
    "```python\n",
    "# Plot distribution of var1\n",
    "plt.subplot(3, 1, 1); sns.distplot(data['var1'])\n",
    "\n",
    "# Plot distribution of var2\n",
    "plt.subplot(3,1,2); sns.distplot(data['var2'])\n",
    "\n",
    "\n",
    "# Plot distribution of var3\n",
    "plt.subplot(3,1,3); sns.distplot(data['var3'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "### Manage skewness\n",
    "We've loaded the same dataset named data. Now your goal will be to remove skewness from var2 and var3 as they had a non-symmetric distribution as you've seen in the previous exercise plot. You will visualize them to make sure the problem is solved!\n",
    "\n",
    "Libraries pandas, numpy, seaborn and matplotlib.pyplot have been loaded as pd, np, sns and plt respectively. Feel free to explore the dataset in the console.\n",
    "```python\n",
    "# Apply log transformation to var2\n",
    "data['var2_log'] = np.log(data['var2'])\n",
    "\n",
    "# Apply log transformation to var3\n",
    "data['var3_log'] = np.log(data['var3'])\n",
    "# Create a subplot of the distribution of var2_log\n",
    "plt.subplot(2, 1, 1); sns.distplot(data['var2_log'])\n",
    "\n",
    "# Create a subplot of the distribution of var3_log\n",
    "plt.subplot(2, 1, 2); sns.distplot(data['var3_log'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "### Center and scale manually\n",
    "We've loaded the same dataset named data. Now your goal will be to center and scale them manually.\n",
    "\n",
    "Libraries pandas, numpy, seaborn and matplotlib.pyplot have been loaded as pd, np, sns and plt respectively. Feel free to explore the dataset in the console.\n",
    "```python\n",
    "# Center the data by subtracting average values from each entry\n",
    "data_centered = data - data.mean()\n",
    "\n",
    "# Scale the data by dividing each entry by standard deviation\n",
    "data_scaled = data / data.std()\n",
    "\n",
    "# Normalize the data by applying both centering and scaling\n",
    "data_normalized = (data - data.mean()) / data.std()\n",
    "\n",
    "# Print summary statistics to make sure average is zero and standard deviation is one\n",
    "print(data_normalized.describe().round(2))\n",
    "\n",
    "```\n",
    "### Center and scale with StandardScaler()\n",
    "We've loaded the same dataset named data. Now your goal will be to center and scale them with StandardScaler from sklearn library.\n",
    "\n",
    "Libraries pandas, numpy, seaborn and matplotlib.pyplot have been loaded as pd, np, sns and plt respectively. We have also imported the StandardScaler.\n",
    "\n",
    "Feel free to explore the dataset in the console.\n",
    "```python\n",
    "# Initialize a scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler\n",
    "scaler.fit(data)\n",
    "\n",
    "# Scale and center the data\n",
    "data_normalized = scaler.transform(data)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "data_normalized = pd.DataFrame(data_normalized, index=data.index, columns=data.columns)\n",
    "\n",
    "# Print summary statistics\n",
    "print(data_normalized.describe().round(2))\n",
    "\n",
    "```\n",
    "### Visualize RFM distributions\n",
    "We have loaded the dataset with RFM values you calculated previously as datamart_rfm. You will now explore their distributions.\n",
    "\n",
    "The plt.subplot(...) call before the seaborn function call allows you to plot several subplots in one chart, you do not have to change it.\n",
    "\n",
    "Libraries seaborn and matplotlib.pyplot have been loaded as sns and plt respectively. Feel free to explore the dataset in the console.\n",
    "```python\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_rfm['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3,1,2); sns.distplot(datamart_rfm['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3,1,3); sns.distplot(datamart_rfm['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "### Pre-process RFM data\n",
    "We have loaded the dataset with RFM values you calculated previously as datamart_rfm. Since the variables are skewed and are on different scales, you will now un-skew and normalize them.\n",
    "\n",
    "The pandas library is loaded as pd, and numpy as np. Take some time to explore the datamart_rfm in the console.\n",
    "```python\n",
    "# Unskew the data\n",
    "datamart_log = np.log(datamart_rfm)\n",
    "\n",
    "# Initialize a standard scaler and fit it\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(datamart_log)\n",
    "\n",
    "# Scale and center the data\n",
    "datamart_normalized = scaler.transform(datamart_log)\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "datamart_normalized = pd.DataFrame(data=datamart_normalized, index=datamart_rfm.index, columns=datamart_rfm.columns)\n",
    "\n",
    "```\n",
    "## Visualize the normalized variables\n",
    "Great work! Now you will plot the normalized and unskewed variables to see the difference in the distribution as well as the range of the values. The datamart_normalized dataset from the previous exercise is loaded.\n",
    "\n",
    "The plt.subplot(...) call before the seaborn function call allows you to plot several subplots in one chart, you do not have to change it.\n",
    "\n",
    "Libraries seaborn and matplotlib.pyplot have been loaded as sns and plt respectively. Feel free to explore the datamart_normalized in the console.\n",
    "```python\n",
    "# Plot recency distribution\n",
    "plt.subplot(3, 1, 1); sns.distplot(datamart_normalized['Recency'])\n",
    "\n",
    "# Plot frequency distribution\n",
    "plt.subplot(3, 1, 2); sns.distplot(datamart_normalized['Frequency'])\n",
    "\n",
    "# Plot monetary value distribution\n",
    "plt.subplot(3, 1, 3); sns.distplot(datamart_normalized['MonetaryValue'])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "### Run KMeans\n",
    "You will now build a 3 clusters with k-means clustering. We have loaded the pre-processed RFM dataset as datamart_normalized. We have also loaded the pandas library as pd.\n",
    "\n",
    "You can explore the dataset in the console to get familiar with it.\n",
    "```python\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=1)\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "```\n",
    "### Assign labels to raw data\n",
    "You will now analyze the average RFM values of the three clusters you've created in the previous exercise. We have loaded the raw RFM dataset as datamart_rfm, and the cluster labels as cluster_labels. pandas is available as pd.\n",
    "\n",
    "Feel free to explore the date in the console.\n",
    "```python\n",
    "# Create a DataFrame by adding a new cluster label column\n",
    "datamart_rfm_k3 = datamart_rfm.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group the data by cluster\n",
    "grouped = datamart_rfm_k3.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFM values and segment sizes per cluster value\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': ['mean', 'count']\n",
    "  }).round(1)\n",
    "\n",
    "```\n",
    "### Calculate sum of squared errors\n",
    "In this exercise, you will calculate the sum of squared errors for different number of clusters ranging from 1 to 20. In this example we are using a custom created dataset to get a cleaner elbow read.\n",
    "\n",
    "We have loaded the normalized version of data as data_normalized. The KMeans module from scikit-learn is already imported. Also, we have initialized an empty dictionary to store sum of squared errors as sse = {}.\n",
    "\n",
    "Feel free to explore the data in the console.\n",
    "```python\n",
    "# Fit KMeans and calculate SSE for each k\n",
    "for k in range(1, 21):\n",
    "  \n",
    "    # Initialize KMeans with k clusters\n",
    "    kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "    \n",
    "    # Fit KMeans on the normalized dataset\n",
    "    kmeans.fit(data_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of dictionary\n",
    "    sse[k] = kmeans.inertia_\n",
    "    print(sse)\n",
    "\n",
    "```\n",
    "### Plot sum of squared errors\n",
    "Now you will plot the sum of squared errors for each value of k and identify if there is an elbow. This will guide you towards the recommended number of clusters to use.\n",
    "\n",
    "The sum of squared errors is loaded as a dictionary called sse from the previous exercise. matplotlib.pyplot was loaded as plt, and seaborn as sns.\n",
    "\n",
    "You can explore the dictionary in the console.\n",
    "```python\n",
    "# Add the plot title \"The Elbow Method\"\n",
    "plt.title('The Elbow Method')\n",
    "\n",
    "# Add X-axis label \"k\"\n",
    "plt.xlabel('k')\n",
    "\n",
    "# Add Y-axis label \"SSE\"\n",
    "plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each key in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "### Prepare data for the snake plot\n",
    "Now you will prepare data for the snake plot. You will use the 3-cluster RFM segmentation solution you have built previously. You will transform the normalized RFM data into a long format by \"melting\" the metric columns into two columns - one for the name of the metric, and another for the actual numeric value.\n",
    "\n",
    "We have loaded the normalized RFM data with the cluster labels already assigned. It is loaded as apandas DataFrame named datamart_normalized. Also, pandas is imported as pd.\n",
    "\n",
    "Explore the datamart_normalized in the console before you begin the exercise to get a good sense of its structure!\n",
    "```python\n",
    "# Melt the normalized dataset and reset the index\n",
    "datamart_melt = pd.melt(\n",
    "  \t\t\t\t\tdatamart_normalized.reset_index(), \n",
    "                        \n",
    "# Assign CustomerID and Cluster as ID variables\n",
    "                    id_vars =['CustomerID', 'Cluster'],\n",
    "\n",
    "# Assign RFM values as value variables\n",
    "                    value_vars =['Recency', 'Frequency', 'MonetaryValue'], \n",
    "                        \n",
    "# Name the variable and value\n",
    "                    var_name ='Metric', value_name='Value'\n",
    "\t\t\t\t\t)\n",
    "\t\t\t\t\t\n",
    "\t\t\t\t\t\n",
    "print(datamart_melt.head())\n",
    "\n",
    "\n",
    " CustomerID  Cluster   Metric     Value\n",
    "0       12747        2  Recency -2.002202\n",
    "1       12748        2  Recency -2.814518\n",
    "2       12749        2  Recency -1.789490\n",
    "3       12820        2  Recency -1.789490\n",
    "4       12822        1  Recency  0.337315\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "### Visualize snake plot\n",
    "Good work! You will now use the melted dataset to build the snake plot. The melted data is loaded as datamart_melt.\n",
    "\n",
    "The seaborn library is loaded as sns and matplotlib.pyplot is available as plt.\n",
    "\n",
    "You can use the console to explore the melted dataset.\n",
    "```python\n",
    "# Add the plot title\n",
    "plt.title('Snake plot of normalized variables')\n",
    "\n",
    "# Add the x axis label\n",
    "plt.xlabel('Metric')\n",
    "\n",
    "# Add the y axis label\n",
    "plt.ylabel('Value')\n",
    "\n",
    "# Plot a line for each value of the cluster variable\n",
    "sns.lineplot(data=datamart_melt, x='Metric', y='Value', hue='Cluster')\n",
    "plt.show()\n",
    "```\n",
    "### Calculate relative importance of each attribute\n",
    "Now you will calculate the relative importance of the RFM values within each cluster.\n",
    "\n",
    "We have loaded datamart_rfm with raw RFM values, and datamart_rfm_k3 which has raw RFM values and the cluster labels stored as Cluster. The pandas library is also loaded as pd.\n",
    "\n",
    "Feel free to explore the datasets in the console.\n",
    "\n",
    "```python\n",
    "# Calculate average RFM values for each cluster\n",
    "cluster_avg = datamart_rfm_k3.groupby(['Cluster']).mean() \n",
    "\n",
    "# Calculate average RFM values for the total customer population\n",
    "population_avg = datamart_rfm.mean()\n",
    "\n",
    "# Calculate relative importance of cluster's attribute value compared to population\n",
    "relative_imp = cluster_avg / population_avg - 1\n",
    "\n",
    "# Print relative importance scores rounded to 2 decimals\n",
    "print(relative_imp.round(2))\n",
    "\n",
    "        Recency  Frequency  MonetaryValue\n",
    "Cluster                                   \n",
    "0           0.84      -0.84          -0.86\n",
    "1          -0.15      -0.35          -0.42\n",
    "2          -0.82       1.67           1.82\n",
    "\n",
    "```\n",
    "### Plot relative importance heatmap\n",
    "Great job! Now you will build a heatmap visualizing the relative scores for each cluster.\n",
    "\n",
    "We have loaded the relative importance scores as relative_imp. The seaborn library is loaded as sns and the pyplot module from matplotlib is available as plt.\n",
    "```python\n",
    "# Initialize a plot with a figure size of 8 by 2 inches \n",
    "plt.figure(figsize=(8,2 ))\n",
    "\n",
    "# Add the plot title\n",
    "plt.title('Relative importance of attributes')\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(data=relative_imp, annot=True, fmt='.2f', cmap='RdYlGn')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "### End-to-End Segmentation Solution\n",
    "```python\n",
    "# Import StandardScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Apply log transformation\n",
    "datamart_rfmt_log = np.log(datamart_rfmt)\n",
    "\n",
    "# Initialize StandardScaler and fit it \n",
    "scaler = StandardScaler(); scaler.fit(datamart_rfmt_log)\n",
    "\n",
    "# Transform and store the scaled data as datamart_rfmt_normalized\n",
    "datamart_rfmt_normalized = scaler.transform(datamart_rfmt_log)\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Fit KMeans and calculate SSE for each k between 1 and 10\n",
    "for k in range(1, 11):\n",
    "  \n",
    "    # Initialize KMeans with k clusters and fit it \n",
    "    kmeans = KMeans(n_clusters=k, random_state=1).fit(datamart_rfmt_normalized)\n",
    "    \n",
    "    # Assign sum of squared distances to k element of the sse dictionary\n",
    "    sse[k] = kmeans.inertia_   \n",
    "\n",
    "# Add the plot title, x and y axis labels\n",
    "plt.title('The Elbow Method'); plt.xlabel('k'); plt.ylabel('SSE')\n",
    "\n",
    "# Plot SSE values for each k stored as keys in the dictionary\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```python\n",
    "# Import KMeans \n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Initialize KMeans\n",
    "kmeans = KMeans(n_clusters=4, random_state=1) \n",
    "\n",
    "# Fit k-means clustering on the normalized data set\n",
    "kmeans.fit(datamart_rfmt_normalized)\n",
    "\n",
    "# Extract cluster labels\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Create a new DataFrame by adding a cluster label column to datamart_rfmt\n",
    "datamart_rfmt_k4 = datamart_rfmt.assign(Cluster=cluster_labels)\n",
    "\n",
    "# Group by cluster\n",
    "grouped = datamart_rfmt_k4.groupby(['Cluster'])\n",
    "\n",
    "# Calculate average RFMT values and segment sizes for each cluster\n",
    "grouped.agg({\n",
    "    'Recency': 'mean',\n",
    "    'Frequency': 'mean',\n",
    "    'MonetaryValue': 'mean',\n",
    "    'Tenure': ['mean', 'count']\n",
    "  }).round(1)\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

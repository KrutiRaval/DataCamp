{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Fundamentals in Python\n",
    "We'll learn Natural Language Processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.\n",
    "\n",
    "## 1.Regular expressions & word tokenization\n",
    "### 1.1.Introduction to regular expressions\n",
    "**What is the Natural Language Processing (NLP)**\n",
    "* Field of study focused on making sense of language\n",
    "    * Using statistics and computers\n",
    "* You will learn the basics of NLP\n",
    "    * Topic identification\n",
    "    * Text classification\n",
    "* NLP applications include:\n",
    "    * Chatbots\n",
    "    * Translation\n",
    "    * Sentiment analysis\n",
    "**What exactly are regular expressions?**\n",
    "* Strings with a special syntax\n",
    "* Allow us to match patterns in other strings\n",
    "* Applications of regular expressions:\n",
    "    * Find all web links in a document\n",
    "    * Parse email addresses, remove/replace unwanted characters\n",
    "    \n",
    "```python\n",
    "#import library\n",
    "import re\n",
    "#matches a pattern with a string\n",
    "re.match('abc', 'abcdef')\n",
    "```\n",
    "it's takes the first argument as pattern and second argument as a string and returns a match object\n",
    "```python\n",
    "##OUTPUT\n",
    "<_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
    "\n",
    "#it is used for word matches\n",
    "word_regex = '\\w+'\n",
    "In [4]: re.match(word_regex, 'hi there!') \n",
    "###OUTPUT #retuns first word it found\n",
    "<_sre.SRE_Match object; span=(0, 2), match='hi'>\n",
    "```\n",
    "\n",
    "### Common regex patterns\n",
    "|Pattern|Matches|Example|\n",
    "|---|---|---|\n",
    "|\\w+|word|'Magic'|\n",
    "|\\d|digit|9|\n",
    "|\\s|space|''|\n",
    "|.*|wildcard|match anything|\n",
    "|+or*|greedy match|repeats of single letter|\n",
    "|\\S|not Space|'no_spaces'|\n",
    "|[a-z]|lowercase group|'abcdefg'|\n",
    "\n",
    "### Python's re Module\n",
    "* `re` module\n",
    "* `split:` split a string on regex\n",
    "* `findall:` find all patterns in a string\n",
    "* `search:` search for a pattern\n",
    "* `match:` match an entire string or substring based on a pattern\n",
    "* Pattern first, and the string second\n",
    "* May return an iterator, string, or match object\n",
    "\n",
    "```python\n",
    "In [5]: re.split('\\s+', 'Split on spaces.')\n",
    "Out[5]: ['Split', 'on', 'spaces.']\n",
    "   \n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "**Practicing regular expressions: re.split() and re.findall()**\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "#Write a pattern called sentence_endings to match sentence endings (., ?, and !).\n",
    "sentence_endings = r\"[.!?]\"\n",
    "print(re.split(sentence_endings, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_word = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_word, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "# Split my_string on spaces and print the result\n",
    "split_word = r\"\\s+\"\n",
    "print(re.split(split_word, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "#find the digit in the my_string\n",
    "digit = r\"\\d+\"\n",
    "print(re.findall(digit, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introduction to tokenization\n",
    "\n",
    "**What is tokenization?**\n",
    "* Turning a string or document into tokens (smaller chunks)\n",
    "* One step in preparing a text for NLP\n",
    "* Many different theories and rules\n",
    "* You can create your own rules using regular expressions\n",
    "* Some examples:\n",
    "    * Breaking out words or sentences\n",
    "    * Separating punctuation\n",
    "    * Separating all hashtags in a tweet\n",
    "  \n",
    "**nltk library**\n",
    "* nltk: natural language toolkit\n",
    "\n",
    "```python\n",
    "In [1]: from nltk.tokenize import word_tokenize\n",
    "\n",
    "In [2]: word_tokenize(\"Hi there!\") \n",
    "Out[2]: ['Hi', 'there', '!']\n",
    "```\n",
    "**Why tokenize?**\n",
    "* Easier to map part of speech\n",
    "* Matching common words\n",
    "* Removing unwanted tokens\n",
    "* \"I don't like Sam's shoes.\"\n",
    "    * \"I\", \"do\", \"n't\", \"like\", \"Sam\", \"'s\", \"shoes\", \".\"\n",
    "\n",
    "**Other nltk tokenizers**\n",
    "* `sent_tokenize:` tokenize a document into sentences\n",
    "\n",
    "* `regexp_tokenize:` tokenize a string or document based on a regular expression pattern\n",
    "\n",
    "* `TweetTokenizer:` special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points!!!\n",
    "\n",
    "**More regex practice**\n",
    "* Difference between re.search() and re.match()\n",
    "* when use same pattern ans string on re.match() and re.search()\n",
    "\n",
    "```python\n",
    "In [1]: import re\n",
    "\n",
    "In [2]: re.match('abc', 'abcde')\n",
    "Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
    "\n",
    "In [3]: re.search('abc', 'abcde')\n",
    "Out[3]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
    "\n",
    "In [4]: re.match('cd', 'abcde')\n",
    "\n",
    "In [5]: re.search('cd', 'abcde')\n",
    "Out[5]: <_sre.SRE_Match object; span=(2, 4), match='cd'>\n",
    "```\n",
    "\n",
    "* re.match() find the word from begining, if the match is not in the beginning match is not able to find anyting\n",
    "* re.search() search the entrie string \n",
    "\n",
    "### Examples\n",
    "\n",
    "**Word tokenization with NLTK**\n",
    "\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been found in  scene_one. \n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one =\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aysbt/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'one', 'ounce', 'in', 'halves', 'right', 'tropical', 'does', 'mean', 'I', 'times', 'who', 'be', 'your', 'empty', 'plover', 'temperate', 'maintain', '...', 'covered', \"'m\", 'ratios', 'ridden', 'England', 'Found', '[', 'But', ']', 'velocity', 'back', 'We', 'order', 'why', 'Arthur', 'grips', 'snows', 'matter', 'winter', 'It', 'beat', 'No', 'Who', 'at', '#', 'Will', 'Saxons', 'use', 'bird', 'land', 'minute', 'it', 'Am', 'sun', '?', 'They', 'Pull', 'migrate', 'grip', 'You', 'yeah', 'Ridden', 'Well', 'maybe', 'its', 'Supposing', 'together', 'Court', 'What', \"'ve\", 'King', 'needs', 'Please', 'carrying', 'sovereign', 'since', 'carried', 'and', 'pound', 'agree', 'kingdom', 'Uther', 'with', \"'re\", 'am', 'yet', 'trusty', 'could', 'them', '1', 'if', 'lord', 'dorsal', 'husk', 'Britons', 'you', 'weight', 'under', 'Oh', 'do', 'line', 'that', 'point', 'bangin', '.', 'clop', 'he', \"'em\", 'five', 'an', 'length', 'That', 'just', 'In', 'strand', 'strangers', 'SOLDIER', 'castle', 'of', 'The', 'may', 'non-migratory', 'other', 'wings', 'zone', 'held', 'warmer', 'goes', 'son', 'fly', 'me', 'second', 'climes', 'servant', 'then', 'south', 'using', 'SCENE', 'forty-three', 'anyway', 'must', 'these', 'simple', 'Patsy', 'speak', 'or', 'but', '!', 'European', 'search', 'wind', 'suggesting', 'court', 'they', 'go', 'Whoa', 'my', \"'d\", 'coconut', 'will', 'Are', 'Not', 'guiding', 'get', 'have', 'ARTHUR', 'So', 'here', 'question', 'by', 'every', ',', 'knights', 'creeper', 'Halt', 'African', 'ask', 'this', 'Mercea', 'interested', 'where', 'join', 'course', 'there', 'on', 'swallows', 'wants', ':', 'feathers', \"n't\", 'master', 'a', '2', 'house', 'defeator', 'air-speed', 'coconuts', 'swallow', 'horse', 'are', 'Camelot', 'Yes', 'found', 'KING', 'two', 'to', 'tell', 'Listen', 'carry', 'our', 'bring', 'Wait', \"'\", 'Where', 'through', 'martin', 'got', 'is', 'breadth', '--', \"'s\", 'not', 'A', 'from', 'the', 'all', 'Pendragon', 'seek'}\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "#Use re.search() to search for the first occurance of the word \"coconuts\" in scene_one. Store the result in match\n",
    "match = re.search('coconuts', scene_one)\n",
    "#Print the start and end indexes of match using its .start() and .end() methods, respectively\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: patter\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print i\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Advanced tokenization with NLTK and regex\n",
    "**Regex groups using or \"|\"**\n",
    "* OR is represented using |\n",
    "* You can define a group using ()\n",
    "* You can define explicit character ranges using []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'has', '11', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# ('(\\d+|\\w+)') finds all word or digits\n",
    "mathes_digits_and_word = ('(\\d+|\\w+)')\n",
    "print(re.findall(mathes_digits_and_word,'He has 11 cats.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex ranges and groups\n",
    "\n",
    "|pattern|\tmatches|\texample|\n",
    "|---|---|---|\n",
    "|[A-Za-z]+|upper and lowercase English alphabet\t|'ABCDEFghijk'|\n",
    "|[0-9]|\tnumbers from 0 to 9\t|9|\n",
    "|[A-Za-z\\-\\.]+\t|upper and lowercase English alphabet, - and .\t|'My-Website.com'\n",
    "|(a-z)|\ta, - and z\t|'a-z'|\n",
    "|(\\s+l,)|\tspaces or a comma\t|', '|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>\n"
     ]
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "print(re.match('[a-z0-9 ]+', my_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "#### Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets =['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "#import TweetTokenizer\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([@dDataCamp]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "```python\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')```\n",
    "                                                                         \n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text ='Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "print(word_tokenize(german_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Über']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only capital words\n",
    "capital_worlds = r'[A-ZÜ]\\w+'\n",
    "print(regexp_tokenize(german_text, capital_worlds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only emoji\n",
    "#define the unicode for emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Charting word length with NLTK\n",
    "Getting started with matplotlib\n",
    "Charting library used by many open source Python projects\n",
    "Straightforward functionality with lots of options\n",
    "Histograms\n",
    "Bar charts\n",
    "Line charts\n",
    "Scatter plots\n",
    "... and also advanced functionality like 3D graphs and animations!\n",
    "#### Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEKCAYAAADpfBXhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAEadJREFUeJzt3XuQnXV9x/H3R5J6AZSO2VYaElaF0XoNkFIYxMFLLQoDzogFp96oTkYHK0y1FfkDKv8oYwdbxZGJgoKiYEFsKrFoRxRsEUli5BbopBSHDDiJokC8oMFv/zhPZtZll3N295wc8sv7NXNmn8vvPM/3mU0++9vfPs/vpKqQJLXlSeMuQJI0fIa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGLxnXiJUuW1OTk5LhOL0m7pfXr1/+kqib6tRtbuE9OTrJu3bpxnV6SdktJfjRIO4dlJKlBhrskNchwl6QGGe6S1CDDXZIa1DfckzwlyfeT/DDJ7Uk+NEObJye5IsnmJDclmRxFsZKkwQzSc38EeGVVvRRYARyb5Ihpbd4B/KyqDgI+Bpw33DIlSXPRN9yrZ3u3urh7Tf9svhOBS7rlK4FXJcnQqpQkzclAY+5J9kqyEdgKfLOqbprWZClwL0BV7QAeBJ45zEIlSYMb6AnVqnoUWJFkP+DqJC+qqtumNJmpl/6YT95OsgpYBbB8+fJ5lCu1bfLMa8Zy3ns+ctxYzqvRmdPdMlX1c+DbwLHTdm0BlgEkWQQ8A3hghvevrqqVVbVyYqLv1AiSpHka5G6Zia7HTpKnAq8G7pzWbA3wtm75JOBbVfWYnrskadcYZFhmf+CSJHvR+2Hw5ar6WpJzgXVVtQa4CPh8ks30euynjKxiSVJffcO9qm4BDplh+9lTln8NvHG4pUmS5ssnVCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDWob7gnWZbkuiSbktye5PQZ2hyT5MEkG7vX2aMpV5I0iEUDtNkBvK+qNiTZF1if5JtVdce0djdU1fHDL1GSNFd9e+5VdX9VbeiWHwY2AUtHXZgkaf7mNOaeZBI4BLhpht1HJvlhkq8neeEs71+VZF2Sddu2bZtzsZKkwQwc7kn2Aa4Czqiqh6bt3gAcWFUvBT4BfHWmY1TV6qpaWVUrJyYm5luzJKmPgcI9yWJ6wX5ZVX1l+v6qeqiqtnfLa4HFSZYMtVJJ0sAGuVsmwEXApqo6f5Y2z+rakeTw7rg/HWahkqTBDXK3zFHAW4Bbk2zstp0FLAeoqguBk4B3J9kB/Ao4papqBPVKkgbQN9yr6rtA+rS5ALhgWEVJkhbGJ1QlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUF9wz3JsiTXJdmU5PYkp8/QJkk+nmRzkluSHDqaciVJg1g0QJsdwPuqakOSfYH1Sb5ZVXdMafNa4ODu9efAp7qvkqQx6Ntzr6r7q2pDt/wwsAlYOq3ZicCl1fM9YL8k+w+9WknSQOY05p5kEjgEuGnarqXAvVPWt/DYHwCSpF1kkGEZAJLsA1wFnFFVD03fPcNbaoZjrAJWASxfvnwOZf6+yTOvmfd7F+qejxw3tnNL0qAG6rknWUwv2C+rqq/M0GQLsGzK+gHAfdMbVdXqqlpZVSsnJibmU68kaQCD3C0T4CJgU1WdP0uzNcBbu7tmjgAerKr7h1inJGkOBhmWOQp4C3Brko3dtrOA5QBVdSGwFngdsBn4JXDq8EuVJA2qb7hX1XeZeUx9apsCThtWUZKkhfEJVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ3qG+5JLk6yNclts+w/JsmDSTZ2r7OHX6YkaS4WDdDmc8AFwKWP0+aGqjp+KBVJkhasb8+9qq4HHtgFtUiShmRYY+5HJvlhkq8neeFsjZKsSrIuybpt27YN6dSSpOmGEe4bgAOr6qXAJ4CvztawqlZX1cqqWjkxMTGEU0uSZrLgcK+qh6pqe7e8FlicZMmCK5MkzduCwz3Js5KkWz68O+ZPF3pcSdL89b1bJsmXgGOAJUm2AOcAiwGq6kLgJODdSXYAvwJOqaoaWcWSpL76hntVvanP/gvo3SopSXqC8AlVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWpQ33BPcnGSrUlum2V/knw8yeYktyQ5dPhlSpLmYpCe++eAYx9n/2uBg7vXKuBTCy9LkrQQfcO9qq4HHnicJicCl1bP94D9kuw/rAIlSXM3jDH3pcC9U9a3dNskSWOyaAjHyAzbasaGySp6QzcsX758CKfec0yeec3Yzn3PR44b27mlUWn9/9Qweu5bgGVT1g8A7pupYVWtrqqVVbVyYmJiCKeWJM1kGOG+Bnhrd9fMEcCDVXX/EI4rSZqnvsMySb4EHAMsSbIFOAdYDFBVFwJrgdcBm4FfAqeOqlhJ0mD6hntVvanP/gJOG1pFkqQF8wlVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRoo3JMcm+SuJJuTnDnD/rcn2ZZkY/d65/BLlSQNalG/Bkn2Aj4J/AWwBbg5yZqqumNa0yuq6j0jqFGSNEeD9NwPBzZX1d1V9RvgcuDE0ZYlSVqIQcJ9KXDvlPUt3bbp3pDkliRXJlk204GSrEqyLsm6bdu2zaNcSdIgBgn3zLCtpq3/OzBZVS8B/hO4ZKYDVdXqqlpZVSsnJibmVqkkaWCDhPsWYGpP/ADgvqkNquqnVfVIt/pp4LDhlCdJmo9Bwv1m4OAkz07yB8ApwJqpDZLsP2X1BGDT8EqUJM1V37tlqmpHkvcA1wJ7ARdX1e1JzgXWVdUa4L1JTgB2AA8Abx9hzZKkPvqGO0BVrQXWTtt29pTlDwIfHG5pkqT58glVSWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWrQQOGe5NgkdyXZnOTMGfY/OckV3f6bkkwOu1BJ0uD6hnuSvYBPAq8FXgC8KckLpjV7B/CzqjoI+Bhw3rALlSQNbpCe++HA5qq6u6p+A1wOnDitzYnAJd3ylcCrkmR4ZUqS5mKQcF8K3DtlfUu3bcY2VbUDeBB45jAKlCTN3aIB2szUA695tCHJKmBVt7o9yV0DnH8mS4CfzPO9C5LxDTh5zXuGsVzzGL/HsAd+n3Pegq75wEEaDRLuW4BlU9YPAO6bpc2WJIuAZwAPTD9QVa0GVg9S2ONJsq6qVi70OLsTr3nP4DXvGXbFNQ8yLHMzcHCSZyf5A+AUYM20NmuAt3XLJwHfqqrH9NwlSbtG3557Ve1I8h7gWmAv4OKquj3JucC6qloDXAR8Pslmej32U0ZZtCTp8Q0yLENVrQXWTtt29pTlXwNvHG5pj2vBQzu7Ia95z+A17xlGfs1x9ESS2uP0A5LUoN0q3JNcnGRrktvGXcuukmRZkuuSbEpye5LTx13TqCV5SpLvJ/lhd80fGndNu0KSvZL8IMnXxl3LrpLkniS3JtmYZN246xm1JPsluTLJnd3/6SNHdq7daVgmycuB7cClVfWicdezKyTZH9i/qjYk2RdYD7y+qu4Yc2kj0z3dvHdVbU+yGPgucHpVfW/MpY1Ukr8DVgJPr6rjx13PrpDkHmBlVe0R97knuQS4oao+0919+LSq+vkozrVb9dyr6npmuH++ZVV1f1Vt6JYfBjbx2CeEm1I927vVxd1r9+mFzEOSA4DjgM+MuxaNRpKnAy+nd3chVfWbUQU77GbhvqfrZts8BLhpvJWMXjdEsRHYCnyzqlq/5n8G/gH43bgL2cUK+EaS9d0T7C17DrAN+Gw3/PaZJHuP6mSG+24iyT7AVcAZVfXQuOsZtap6tKpW0Hsi+vAkzQ7DJTke2FpV68ddyxgcVVWH0pt19rRu6LVVi4BDgU9V1SHAL4DHTKE+LIb7bqAbd74KuKyqvjLuenal7tfWbwPHjrmUUToKOKEbf74ceGWSL4y3pF2jqu7rvm4FrqY3C22rtgBbpvwWeiW9sB8Jw/0Jrvvj4kXApqo6f9z17ApJJpLs1y0/FXg1cOd4qxqdqvpgVR1QVZP0nu7+VlW9ecxljVySvbubBOiGJ14DNHsnXFX9GLg3yfO6Ta8CRnZjxEBPqD5RJPkScAywJMkW4Jyqumi8VY3cUcBbgFu7MWiAs7qnhlu1P3BJ90ExTwK+XFV7zO2Be5A/Bq7uPvphEfDFqvqP8ZY0cn8LXNbdKXM3cOqoTrRb3QopSRqMwzKS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3LXLJdnev9Wcj7kiyeumrP9jkvcP+RzP72Yv/EGS5w7z2LOcb3JPmgFVw2W4qxUrgNf1bbUwrwf+raoOqar/HfbBu/v6paEw3DVWSf4+yc1Jbtk5b3vXY92U5NPdfO7f6J5UJcmfdW1vTPLRJLd1D4ScC5zc9axP7g7/giTfTnJ3kvd27987yTXdXPG3TWk7taYVSb7XnefqJH/Y/VZwBvDOJNdNa/9XSc7vlk9Pcne3/Nwk3+2WX9X1+G/tPpfgyd32e5Kc3bV7Y5LDutpuBE6bco4XdnPcb+zqOniI3wY1yHDX2CR5DXAwvflEVgCHTZk46mDgk1X1QuDnwBu67Z8F3lVVRwKPQm/qVOBs4IqqWlFVV3Rtnw/8ZXf8c7o5eo4F7quql3afCTDTE5GXAh+oqpcAt9J7EnotcCHwsap6xbT21wNHd8tHAz9NshR4GXBDkqcAnwNOrqoX03sa891T3v/rqnpZVV3eXd97u+ub6l3Av3STqa2kN0+JNCvDXeP0mu71A2ADvTDe2SP9v6raOd3CemCym29m36r67277F/sc/5qqeqT7IIit9B53vxV4dZLzkhxdVQ9OfUOSZwD7VdV3uk2X0JuDe1bdnCH7dPOkLOvqejm9oL8BeF53Pf8zyzGvmOXcn5/S5kbgrCQfAA6sql/1uXbt4Qx3jVOAD3e97RVVddCUuYIemdLuUXq93czx+I85Rhewh9EL+Q8nOXuetU93I715Qu6iF+hHA0cC/0X/un/RfQ2zfChJVX0ROAH4FXBtklcOoWY1zHDXOF0L/E03Vz1Jlib5o9kaV9XPgIeTHNFtOmXK7oeBffudMMmfAL+sqi8A/8S0KVe7nvzPkuwcZnkL8B36ux54f/f1B8ArgEe6491J7zePgx7vmN30xg8meVm36a+n1P0c4O6q+jiwBnjJADVpD7ZbzQqptlTVN5L8KXBjNzPgduDNdGPps3gH8Okkv6A3z/vOYZXrgDO7mTM//DjvfzHw0SS/A37L74997/Q24MIkT2PwmftuoDckc31VPZrkXrppiqvq10lOBf41ySLgZnrj9zM5Fbg4yS/p/fDb6WTgzUl+C/yY3h+QpVk5K6R2K0n22fn5qknOpPfh4aePuSzpCceeu3Y3xyX5IL1/uz8C3j7ecqQnJnvuktQg/6AqSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGvT/ZTO7H3oUuJgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "#it creates a list of lenght from tokenized words\n",
    "words_lengths = [len(w) for w in words]\n",
    "plt.hist(words_lengths)\n",
    "plt.xlabel('lengths of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "#### Charting practice\n",
    "Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.\n",
    "\n",
    "You have access to the entire script in the variable holy_grail. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FIRST LOAD THE HOLY GRAIL TEXT\n",
    "file_name = 'data/grail.txt'\n",
    "holy_grill = open(file_name, 'r')\n",
    "lines = holy_grill.read()\n",
    "split_line = lines.split('\\n')\n",
    "#print(split_line)\n",
    "#Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1.\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines_re_sub = [re.sub(pattern, '', l) for l in split_line]\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines_re_sub]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "num_of_word_in_line = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "plt.hist(num_of_word_in_line)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple topic identification\n",
    "This chapter will introduce you to topic identification, which you can apply to any text you encounter in the wild. Using basic NLP models, you will identify topics from texts based on term frequencies. You'll experiment and compare two simple methods - bag-of-words and Tf-idf using NLTK and a new library - Gensim\n",
    "### 2.1 Word counts with bag-of-words\n",
    "\n",
    "**Bag-of-words**\n",
    "* Basic method for finding topics in a text\n",
    "* Need to first create tokens using tokenization\n",
    "* ... and then count up all the tokens\n",
    "* The more frequent a word, the more important it might be\n",
    "* Can be a great way to determine the significant words in a tex\n",
    "\n",
    "**Bag-of-words example**\n",
    "* Text: \"The cat is in the box. The cat likes the box. The box is over the cat.\"\n",
    "* Bag of words (stripped punctuation):\n",
    "\n",
    "    * \"The\": 3, \"box\": 3\n",
    "    * \"cat\": 3, \"the\": 3\n",
    "    * \"is\": 2\n",
    "    * \"in\": 1, \"likes\": 1, \"over\": 1\n",
    "from nltk.tokenize import word_tokenize\n",
    "In [2]: from collections import Counter\n",
    "In [3]: Counter(word_tokenize(\n",
    "                \"\"\"The cat is in the box. The cat likes the box. \n",
    "                 The box is over the cat.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 3, 'cat': 3, 'the': 3, 'box': 3, '.': 3, 'is': 2, 'in': 1, 'likes': 1, 'over': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "#it returns each token and frequency of each token as dict object\n",
    "print(Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. \n",
    "                 The box is over the cat.\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'The': 2, 'cat': 2, 'box': 2, '.': 2, 'is': 1, 'in': 1, 'the': 1})\n"
     ]
    }
   ],
   "source": [
    "print(Counter(word_tokenize(\"\"\"The cat is in the box. The cat box.\"\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = '\\'\\'\\'Debugging\\'\\'\\' is the process of finding and resolving of defects that prevent correct operation of computer software or a system.  \\n\\nNumerous books have been written about debugging (see below: #Further reading|Further reading), as it involves numerous aspects, including interactive debugging, control flow, integration testing, Logfile|log files, monitoring (Application monitoring|application, System Monitoring|system), memory dumps, Profiling (computer programming)|profiling, Statistical Process Control, and special design tactics to improve detection while simplifying changes.\\n\\nOrigin\\nA computer log entry from the Mark&nbsp;II, with a moth taped to the page\\n\\nThe terms \"bug\" and \"debugging\" are popularly attributed to Admiral Grace Hopper in the 1940s.[http://foldoc.org/Grace+Hopper Grace Hopper]  from FOLDOC While she was working on a Harvard Mark II|Mark II Computer at Harvard University, her associates discovered a moth stuck in a relay and thereby impeding operation, whereupon she remarked that they were \"debugging\" the system. However the term \"bug\" in the meaning of technical error dates back at least to 1878 and Thomas Edison (see software bug for a full discussion), and \"debugging\" seems to have been used as a term in aeronautics before entering the world of computers. Indeed, in an interview Grace Hopper remarked that she was not coining the term{{Citation needed|date=July 2015}}. The moth fit the already existing terminology, so it was saved.  A letter from J. Robert Oppenheimer (director of the WWII atomic bomb \"Manhattan\" project at Los Alamos, NM) used the term in a letter to Dr. Ernest Lawrence at UC Berkeley, dated October 27, 1944,http://bancroft.berkeley.edu/Exhibits/physics/images/bigscience25.jpg regarding the recruitment of additional technical staff.\\n\\nThe Oxford English Dictionary entry for \"debug\" quotes the term \"debugging\" used in reference to airplane engine testing in a 1945 article in the Journal of the Royal Aeronautical Society. An article in \"Airforce\" (June 1945 p.&nbsp;50) also refers to debugging, this time of aircraft cameras.  Hopper\\'s computer bug|bug was found on September 9, 1947. The term was not adopted by computer programmers until the early 1950s.\\nThe seminal article by GillS. Gill, [http://www.jstor.org/stable/98663 The Diagnosis of Mistakes in Programmes on the EDSAC], Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, Vol. 206, No. 1087 (May 22, 1951), pp. 538-554 in 1951 is the earliest in-depth discussion of programming errors, but it does not use the term \"bug\" or \"debugging\".\\nIn the Association for Computing Machinery|ACM\\'s digital library, the term \"debugging\" is first used in three papers from 1952 ACM National Meetings.Robert V. D. Campbell, [http://portal.acm.org/citation.cfm?id=609784.609786 Evolution of automatic computation], Proceedings of the 1952 ACM national meeting (Pittsburgh), p 29-32, 1952.Alex Orden, [http://portal.acm.org/citation.cfm?id=609784.609793 Solution of systems of linear inequalities on a digital computer], Proceedings of the 1952 ACM national meeting (Pittsburgh), p. 91-95, 1952.Howard B. Demuth, John B. Jackson, Edmund Klein, N. Metropolis, Walter Orvedahl, James H. Richardson, [http://portal.acm.org/citation.cfm?id=800259.808982 MANIAC], Proceedings of the 1952 ACM national meeting (Toronto), p. 13-16 Two of the three use the term in quotation marks.\\nBy 1963 \"debugging\" was a common enough term to be mentioned in passing without explanation on page 1 of the Compatible Time-Sharing System|CTSS manual.[http://www.bitsavers.org/pdf/mit/ctss/CTSS_ProgrammersGuide.pdf The Compatible Time-Sharing System], M.I.T. Press, 1963\\n\\nKidwell\\'s article \\'\\'Stalking the Elusive Computer Bug\\'\\'Peggy Aldrich Kidwell, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?tp=&arnumber=728224&isnumber=15706 Stalking the Elusive Computer Bug], IEEE Annals of the History of Computing, 1998. discusses the etymology of \"bug\" and \"debug\" in greater detail.\\n\\nScope\\nAs software and electronic systems have become generally more complex, the various common debugging techniques have expanded with more methods to detect anomalies, assess impact, and schedule software patches or full updates to a system. The words \"anomaly\" and \"discrepancy\" can be used, as being more neutral terms, to avoid the words \"error\" and \"defect\" or \"bug\" where there might be an implication that all so-called \\'\\'errors\\'\\', \\'\\'defects\\'\\' or \\'\\'bugs\\'\\' must be fixed (at all costs). Instead, an impact assessment can be made to determine if changes to remove an \\'\\'anomaly\\'\\' (or \\'\\'discrepancy\\'\\') would be cost-effective for the system, or perhaps a scheduled new release might render the change(s) unnecessary. Not all issues are life-critical or mission-critical in a system. Also, it is important to avoid the situation where a change might be more upsetting to users, long-term, than living with the known problem(s) (where the \"cure would be worse than the disease\"). Basing decisions of the acceptability of some anomalies can avoid a culture of a \"zero-defects\" mandate, where people might be tempted to deny the existence of problems so that the result would appear as zero \\'\\'defects\\'\\'. Considering the collateral issues, such as the cost-versus-benefit impact assessment, then broader debugging techniques will expand to determine the frequency of anomalies (how often the same \"bugs\" occur) to help assess their impact to the overall system.\\n\\nTools\\nDebugging on video game consoles is usually done with special hardware such as this Xbox (console)|Xbox debug unit intended for developers.\\n\\nDebugging ranges in complexity from fixing simple errors to performing lengthy and tiresome tasks of data collection, analysis, and scheduling updates.  The debugging skill of the programmer can be a major factor in the ability to debug a problem, but the difficulty of software debugging varies greatly with the complexity of the system, and also depends, to some extent, on the programming language(s) used and the available tools, such as \\'\\'debuggers\\'\\'. Debuggers are software tools which enable the programmer to monitor the execution (computers)|execution of a program, stop it, restart it, set breakpoints, and change values in memory. The term \\'\\'debugger\\'\\' can also refer to the person who is doing the debugging.\\n\\nGenerally, high-level programming languages, such as Java (programming language)|Java, make debugging easier, because they have features such as exception handling that make real sources of erratic behaviour easier to spot. In programming languages such as C (programming language)|C or assembly language|assembly, bugs may cause silent problems such as memory corruption, and it is often difficult to see where the initial problem happened. In those cases, memory debugging|memory debugger tools may be needed.\\n\\nIn certain situations, general purpose software tools that are language specific in nature can be very useful.  These take the form of \\'\\'List of tools for static code analysis|static code analysis tools\\'\\'.  These tools look for a very specific set of known problems, some common and some rare, within the source code.  All such issues detected by these tools would rarely be picked up by a compiler or interpreter, thus they are not syntax checkers, but more semantic checkers.  Some tools claim to be able to detect 300+ unique problems. Both commercial and free tools exist in various languages.  These tools can be extremely useful when checking very large source trees, where it is impractical to do code walkthroughs.  A typical example of a problem detected would be a variable dereference that occurs \\'\\'before\\'\\' the variable is assigned a value.  Another example would be to perform strong type checking when the language does not require such.  Thus, they are better at locating likely errors, versus actual errors.  As a result, these tools have a reputation of false positives.  The old Unix \\'\\'Lint programming tool|lint\\'\\' program is an early example.\\n\\nFor debugging electronic hardware (e.g., computer hardware) as well as low-level software (e.g., BIOSes, device drivers) and firmware, instruments such as oscilloscopes, logic analyzers or in-circuit emulator|in-circuit emulators (ICEs) are often used, alone or in combination.  An ICE may perform many of the typical software debugger\\'s tasks on low-level software and firmware.\\n\\nDebugging process \\nNormally the first step in debugging is to attempt to reproduce the problem. This can be a non-trivial task, for example as with Parallel computing|parallel processes or some unusual software bugs. Also, specific user environment and usage history can make it difficult to reproduce the problem.\\n\\nAfter the bug is reproduced, the input of the program may need to be simplified to make it easier to debug. For example, a bug in a compiler can make it Crash (computing)|crash when parsing some large source file. However, after simplification of the test case, only few lines from the original source file can be sufficient to reproduce the same crash. Such simplification can be made manually, using a Divide and conquer algorithm|divide-and-conquer approach. The programmer will try to remove some parts of original test case and check if the problem still exists. When debugging the problem in a Graphical user interface|GUI, the programmer can try to skip some user interaction from the original problem description and check if remaining actions are sufficient for bugs to appear.\\n\\nAfter the test case is sufficiently simplified, a programmer can use a debugger tool to examine program states (values of variables, plus the call stack) and track down the origin of the problem(s). Alternatively, Tracing (software)|tracing can be used. In simple cases, tracing is just a few print statements, which output the values of variables at certain points of program execution.{{citation needed|date=February 2016}}\\n\\n Techniques \\n \\'\\'Interactive debugging\\'\\'\\n \\'\\'{{visible anchor|Print debugging}}\\'\\' (or tracing) is the act of watching (live or recorded) trace statements, or print statements, that indicate the flow of execution of a process. This is sometimes called \\'\\'{{visible anchor|printf debugging}}\\'\\', due to the use of the printf function in C. This kind of debugging was turned on by the command TRON in the original versions of the novice-oriented BASIC programming language. TRON stood for, \"Trace On.\" TRON caused the line numbers of each BASIC command line to print as the program ran.\\n \\'\\'Remote debugging\\'\\' is the process of debugging a program running on a system different from the debugger. To start remote debugging, a debugger connects to a remote system over a network. The debugger can then control the execution of the program on the remote system and retrieve information about its state.\\n \\'\\'Post-mortem debugging\\'\\' is debugging of the program after it has already Crash (computing)|crashed. Related techniques often include various tracing techniques (for example,[http://www.drdobbs.com/tools/185300443 Postmortem Debugging, Stephen Wormuller, Dr. Dobbs Journal, 2006]) and/or analysis of memory dump (or core dump) of the crashed process. The dump of the process could be obtained automatically by the system (for example, when process has terminated due to an unhandled exception), or by a programmer-inserted instruction, or manually by the interactive user.\\n \\'\\'\"Wolf fence\" algorithm:\\'\\' Edward Gauss described this simple but very useful and now famous algorithm in a 1982 article for communications of the ACM as follows: \"There\\'s one wolf in Alaska; how do you find it? First build a fence down the middle of the state, wait for the wolf to howl, determine which side of the fence it is on. Repeat process on that side only, until you get to the point where you can see the wolf.\"<ref name=\"communications of the ACM\">{{cite journal | title=\"Pracniques: The \"Wolf Fence\" Algorithm for Debugging\", | author=E. J. Gauss | year=1982}} This is implemented e.g. in the Git (software)|Git version control system as the command \\'\\'git bisect\\'\\', which uses the above algorithm to determine which Commit (data management)|commit introduced a particular bug.\\n \\'\\'Delta Debugging\\'\\'{{snd}} a technique of automating test case simplification.Andreas Zeller: <cite>Why Programs Fail: A Guide to Systematic Debugging</cite>, Morgan Kaufmann, 2005. ISBN 1-55860-866-4{{rp|p.123}}<!-- for redirect from \\'Saff Squeeze\\' -->\\n \\'\\'Saff Squeeze\\'\\'{{snd}} a technique of isolating failure within the test using progressive inlining of parts of the failing test.[http://www.threeriversinstitute.org/HitEmHighHitEmLow.html Kent Beck, Hit \\'em High, Hit \\'em Low: Regression Testing and the Saff Squeeze]\\n\\nDebugging for embedded systems\\nIn contrast to the general purpose computer software design environment, a primary characteristic of embedded environments is the sheer number of different platforms available to the developers (CPU architectures, vendors, operating systems and their variants). Embedded systems are, by definition, not general-purpose designs: they are typically developed for a single task (or small range of tasks), and the platform is chosen specifically to optimize that application. Not only does this fact make life tough for embedded system developers, it also makes debugging and testing of these systems harder as well, since different debugging tools are needed in different platforms.\\n\\nto identify and fix bugs in the system (e.g. logical or synchronization problems in the code, or a design error in the hardware);\\nto collect information about the operating states of the system that may then be used to analyze the system: to find ways to boost its performance or to optimize other important characteristics (e.g. energy consumption, reliability, real-time response etc.).\\n\\nAnti-debugging\\nAnti-debugging is \"the implementation of one or more techniques within computer code that hinders attempts at reverse engineering or debugging a target process\".<ref name=\"veracode-antidebugging\">{{cite web |url=http://www.veracode.com/blog/2008/12/anti-debugging-series-part-i/ |title=Anti-Debugging Series - Part I |last=Shields |first=Tyler |date=2008-12-02 |work=Veracode |accessdate=2009-03-17}} It is actively used by recognized publishers in copy protection|copy-protection schemas, but is also used by malware to complicate its detection and elimination.<ref name=\"soft-prot\">[http://people.seas.harvard.edu/~mgagnon/software_protection_through_anti_debugging.pdf Software Protection through Anti-Debugging Michael N Gagnon, Stephen Taylor, Anup Ghosh] Techniques used in anti-debugging include:\\nAPI-based: check for the existence of a debugger using system information\\nException-based: check to see if exceptions are interfered with\\nProcess and thread blocks: check whether process and thread blocks have been manipulated\\nModified code: check for code modifications made by a debugger handling software breakpoints\\nHardware- and register-based: check for hardware breakpoints and CPU registers\\nTiming and latency: check the time taken for the execution of instructions\\nDetecting and penalizing debugger<ref name=\"soft-prot\" /><!-- reference does not exist -->\\n\\nAn early example of anti-debugging existed in early versions of Microsoft Word which, if a debugger was detected, produced a message that said: \"The tree of evil bears bitter fruit. Now trashing program disk.\", after which it caused the floppy disk drive to emit alarming noises with the intent of scaring the user away from attempting it again.<ref name=\"SecurityEngineeringRA\">{{cite book | url=http://www.cl.cam.ac.uk/~rja14/book.html | author=Ross J. Anderson | title=Security Engineering | isbn = 0-471-38922-6 | page=684 }}<ref name=\"toastytech\">{{cite web | url=http://toastytech.com/guis/word1153.html | title=Microsoft Word for DOS 1.15}}\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 151), ('the', 150), ('.', 89), ('of', 81), (\"''\", 66), ('to', 63), ('a', 60), ('``', 47), ('in', 44), ('and', 41)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import  Counter\n",
    "\n",
    "# Tokenize the article: tokens\n",
    "tokens = word_tokenize(article)\n",
    "\n",
    "# Convert the tokens into lowercase: lower_tokens\n",
    "lower_tokens =[t.lower() for t in tokens] \n",
    "\n",
    "# Create a Counter with the lowercase tokens: bow_simple\n",
    "bow_tokens = Counter(lower_tokens)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow_tokens.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple text preprocessing\n",
    "**Why preprocess?**\n",
    "   * Helps make for better input data\n",
    "        * When performing machine learning or other statistical methods\n",
    "* Examples:\n",
    "    * Tokenization to create a bag of words\n",
    "    * Lowercasing words\n",
    "* Lemmatization/Stemming\n",
    "    * Shorten words to their root stems\n",
    "* Removing stop words, punctuation, or unwanted tokens\n",
    "* Good to experiment with different approaches\n",
    "\n",
    "**Text preprocessing with Python**\n",
    "\n",
    "* isalpha() method will return True if the string has only alphabetical characters.(this will strip number or punctutaion.)\n",
    "\n",
    "```python\n",
    "\n",
    "from ntlk.corpus import stopwords\n",
    "\n",
    "text = \"\"\"The cat is in the box. The cat likes the box. \n",
    "                  The box is over the cat.\"\"\"\n",
    "#list com[rehensions to tokenize lower case if there is only have alphabetical characters \n",
    "tokens = [w for w in word_tokenize(text.lower()) \n",
    "                  if w.isalpha()]\n",
    "#list comprehentions to remove words that are in the stopword list(means comes built in with NLTK libraries.)\n",
    "no_stops = [t for t in tokens \n",
    "                    if t not in stopwords.words('english')]\n",
    "# we can create counter  to count most 2 common words\n",
    "Counter(no_stops).most_common(2)\n",
    "#Out[]: [('cat', 3), ('box', 3)]\n",
    "```\n",
    "\n",
    "Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. This can be done in a list comprehension (the for-loop inside square brackets to make a list).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'v'))  \n",
    "#> strip\n",
    "\n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  \n",
    "#> stripe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/aysbt/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_stops =['i',\n",
    " 'me',\n",
    " 'my',\n",
    " 'myself',\n",
    " 'we',\n",
    " 'our',\n",
    " 'ours',\n",
    " 'ourselves',\n",
    " 'you',\n",
    " 'your',\n",
    " 'yours',\n",
    " 'yourself',\n",
    " 'yourselves',\n",
    " 'he',\n",
    " 'him',\n",
    " 'his',\n",
    " 'himself',\n",
    " 'she',\n",
    " 'her',\n",
    " 'hers',\n",
    " 'herself',\n",
    " 'it',\n",
    " 'its',\n",
    " 'itself',\n",
    " 'they',\n",
    " 'them',\n",
    " 'their',\n",
    " 'theirs',\n",
    " 'themselves',\n",
    " 'what',\n",
    " 'which',\n",
    " 'who',\n",
    " 'whom',\n",
    " 'this',\n",
    " 'that',\n",
    " 'these',\n",
    " 'those',\n",
    " 'am',\n",
    " 'is',\n",
    " 'are',\n",
    " 'was',\n",
    " 'were',\n",
    " 'be',\n",
    " 'been',\n",
    " 'being',\n",
    " 'have',\n",
    " 'has',\n",
    " 'had',\n",
    " 'having',\n",
    " 'do',\n",
    " 'does',\n",
    " 'did',\n",
    " 'doing',\n",
    " 'a',\n",
    " 'an',\n",
    " 'the',\n",
    " 'and',\n",
    " 'but',\n",
    " 'if',\n",
    " 'or',\n",
    " 'because',\n",
    " 'as',\n",
    " 'until',\n",
    " 'while',\n",
    " 'of',\n",
    " 'at',\n",
    " 'by',\n",
    " 'for',\n",
    " 'with',\n",
    " 'about',\n",
    " 'against',\n",
    " 'between',\n",
    " 'into',\n",
    " 'through',\n",
    " 'during',\n",
    " 'before',\n",
    " 'after',\n",
    " 'above',\n",
    " 'below',\n",
    " 'to',\n",
    " 'from',\n",
    " 'up',\n",
    " 'down',\n",
    " 'in',\n",
    " 'out',\n",
    " 'on',\n",
    " 'off',\n",
    " 'over',\n",
    " 'under',\n",
    " 'again',\n",
    " 'further',\n",
    " 'then',\n",
    " 'once',\n",
    " 'here',\n",
    " 'there',\n",
    " 'when',\n",
    " 'where',\n",
    " 'why',\n",
    " 'how',\n",
    " 'all',\n",
    " 'any',\n",
    " 'both',\n",
    " 'each',\n",
    " 'few',\n",
    " 'more',\n",
    " 'most',\n",
    " 'other',\n",
    " 'some',\n",
    " 'such',\n",
    " 'no',\n",
    " 'nor',\n",
    " 'not',\n",
    " 'only',\n",
    " 'own',\n",
    " 'same',\n",
    " 'so',\n",
    " 'than',\n",
    " 'too',\n",
    " 'very',\n",
    " 's',\n",
    " 't',\n",
    " 'can',\n",
    " 'will',\n",
    " 'just',\n",
    " 'don',\n",
    " 'should',\n",
    " 'now',\n",
    " 'd',\n",
    " 'll',\n",
    " 'm',\n",
    " 'o',\n",
    " 're',\n",
    " 've',\n",
    " 'y',\n",
    " 'ain',\n",
    " 'aren',\n",
    " 'couldn',\n",
    " 'didn',\n",
    " 'doesn',\n",
    " 'hadn',\n",
    " 'hasn',\n",
    " 'haven',\n",
    " 'isn',\n",
    " 'ma',\n",
    " 'mightn',\n",
    " 'mustn',\n",
    " 'needn',\n",
    " 'shan',\n",
    " 'shouldn',\n",
    " 'wasn',\n",
    " 'weren',\n",
    " 'won',\n",
    " 'wouldn',\n",
    " '']\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('debugging', 40), ('system', 25), ('bug', 17), ('software', 16), ('problem', 15), ('tool', 15), ('computer', 14), ('process', 13), ('term', 13), ('debugger', 13)]\n"
     ]
    }
   ],
   "source": [
    "#Import the WordNetLemmatizer class from nltk.stem\n",
    "from nltk.stem import  WordNetLemmatizer\n",
    "#Create a list called alpha_only that iterates through lower_tokens and retains only alphabetical characters\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "#Create another list called no_stops in which you remove all stop words,which are held in english_stops list\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "#Initialize a WordNetLemmatizer object called wordnet_lemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#use its .lemmatize() method on the tokens in no_stops to create a new list called lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "#create a new Counter called bow with the lemmatized words and show the 10 most common tokens.\n",
    "bow = Counter(lemmatized)\n",
    "print(bow.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to gensim\n",
    "* Popular open-source NLP library\n",
    "* Uses top academic models to perform complex tasks\n",
    "    * Building document or word vectors\n",
    "    * Performing topic identification and document comparison\n",
    "\n",
    "*A word embedding or vector* is trained from a large corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "#corpus is a set of text use to help perform NLP\n",
    "In [1]: from gensim.corpora.dictionary import Dictionary\n",
    "In [2]: from nltk.tokenize import word_tokenize\n",
    "#Our document is list of string\n",
    "In [3]: my_documents = ['The movie was about a spaceship and aliens.',\n",
    "   ...:                 'I really liked the movie!',\n",
    "   ...:                 'Awesome action scenes, but boring characters.',\n",
    "   ...:                 'The movie was awful! I hate alien films.',\n",
    "   ...:                 'Space is cool! I liked the movie.',\n",
    "   ...:                 'More space films, please!',]\n",
    "# we only tokenize lowercase\n",
    "In [4]: tokenized_docs = [word_tokenize(doc.lower())for doc in my_documents]\n",
    "#pass the tokenezed document to the Gensim dictionary class. this will create mapping class with an id for each class   \n",
    "In [5]: dictionary = Dictionary(tokenized_docs)\n",
    "# we can check their token id and how offen their in ecah document\n",
    "In [6]: dictionary.token2id\n",
    "Out[6]: \n",
    "{'!': 11,\n",
    " ',': 17,\n",
    " '.': 7,\n",
    " 'a': 2,\n",
    " 'about': 4,\n",
    " #it creaet corpus with token id's and frequency of each token\n",
    " corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
    " #cirpus is a list of list, each list item representing one document. Each document is series of tuple, the first item represnting token id form the dictionary and the second id represending the token frequenciy from thedocumnet\n",
    " In [8]: corpus\n",
    "Out[8]: \n",
    "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
    " [(0, 1), (1, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
    "...\n",
    "]\n",
    " \n",
    "#gensim models can be easily saved, updated, and reused\n",
    "\n",
    " \n",
    " # Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    " #output\n",
    " computer\n",
    "[(0, 88), (23, 11), (24, 2), (39, 1), (41, 2), (55, 22), (56, 1), (57, 1), (58, 1), (59, 3)]\n",
    " \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WordNetLemmatizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-aa1ced43bcfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlower\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenize\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0menglish\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlower\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish_stops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mword_lemmetize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0mlemmatize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_lemmetize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menglish\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlemmatize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WordNetLemmatizer' is not defined"
     ]
    }
   ],
   "source": [
    "### import numpy as np\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "    \n",
    "filename = 'data/NLP/Wikipedi_article/wiki_text_computer.txt'\n",
    "with open (file_name, 'r') as file:\n",
    "    #print(file.read())\n",
    "    tokenize = [w for w in word_tokenize(file.read())]\n",
    "    lower = [l.lower() for l in tokenize if l.isalpha()]\n",
    "    english = [e for e in lower if e not in english_stops]\n",
    "    word_lemmetize = WordNetLemmatizer()\n",
    "    lemmatize = [word_lemmetize.lemmatize(t) for t in english]  \n",
    "    lemmatize = [lemmatize]\n",
    "    #print(lemmatize)\n",
    "    \n",
    "    #using gensim\n",
    "    dictionary = Dictionary(lemmatize)\n",
    "    corpus = [dictionary.doc2bow(word) for word in lemmatize]\n",
    "    #print(corpus)\n",
    "    #save the first documnet of corpus\n",
    "    doc = corpus[0]\n",
    "    # Sort the doc for frequency: bow_doc\n",
    "    bow_doc = sorted(doc, key=lambda w :w[1], reverse=True)\n",
    "    for word_id, word_count in bow_doc[:5]:\n",
    "        print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-idf with gensim\n",
    "\n",
    "**What is tf-idf?**\n",
    "* Term frequency - inverse document frequency\n",
    "* Allows you to determine the most important words in each document\n",
    "* Each corpus may have shared words beyond just stopwords\n",
    "* These words should be down-weighted in importance\n",
    "    * Example from astronomy: \"Sky\"\n",
    "* Ensures most common words don't show up as key words\n",
    "* Keeps document specific frequent words weighted high\n",
    "\n",
    "**Tf-idf formula** \n",
    "\n",
    "$ w_{i,j} = tf_{i,j} * \\log (\\frac{N}{df_i})w $ \n",
    "\n",
    "* $w_{i,j}$ = tf-idf weight for token $i$ in document $j$\n",
    "* $tf_{i,j}$ = number of occurences of token $i$ in document $j$\n",
    "* $df_i$ = number of documents that contain token $i$\n",
    "* $N$ = total number of documents\n",
    "\n",
    "```python\n",
    "In [10]: from gensim.models.tfidfmodel import TfidfModel\n",
    "In [11]: tfidf = TfidfModel(corpus)\n",
    "In [12]: tfidf[corpus[1]]\n",
    "Out[12]: \n",
    "[(0, 0.1746298276735174),\n",
    " (1, 0.1746298276735174),\n",
    " (9, 0.29853166221463673),\n",
    " (10, 0.7716931521027908),\n",
    "...\n",
    "]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "tfidf = TfidfModel(corp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

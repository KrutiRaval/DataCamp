{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Fundamentals in Python\n",
    "We'll learn Natural Language Processing (NLP) basics, such as how to identify and separate words, how to extract topics in a text, and how to build your own fake news classifier. You'll also learn how to use basic libraries such as NLTK, alongside libraries which utilize deep learning to solve common NLP problems. This course will give you the foundation to process and parse text as you move forward in your Python learning.\n",
    "\n",
    "## 1.Regular expressions & word tokenization\n",
    "### 1.1. Introduction to regular expressions\n",
    "**What is the Natural Language Processing (NLP)**\n",
    "* Field of study focused on making sense of language\n",
    "    * Using statistics and computers\n",
    "* You will learn the basics of NLP\n",
    "    * Topic identification\n",
    "    * Text classification\n",
    "* NLP applications include:\n",
    "    * Chatbots\n",
    "    * Translation\n",
    "    * Sentiment analysis\n",
    "**What exactly are regular expressions?**\n",
    "* Strings with a special syntax\n",
    "* Allow us to match patterns in other strings\n",
    "* Applications of regular expressions:\n",
    "    * Find all web links in a document\n",
    "    * Parse email addresses, remove/replace unwanted characters\n",
    "    \n",
    "```python\n",
    "#import library\n",
    "import re\n",
    "#matches a pattern with a string\n",
    "re.match('abc', 'abcdef')\n",
    "```\n",
    "it's takes the first argument as pattern and second argument as a string and returns a match object\n",
    "```python\n",
    "##OUTPUT\n",
    "<_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
    "\n",
    "#it is used for word matches\n",
    "word_regex = '\\w+'\n",
    "In [4]: re.match(word_regex, 'hi there!') \n",
    "###OUTPUT #retuns first word it found\n",
    "<_sre.SRE_Match object; span=(0, 2), match='hi'>\n",
    "```\n",
    "\n",
    "### Common regex patterns\n",
    "|Pattern|Matches|Example|\n",
    "|---|---|---|\n",
    "|\\w+|word|'Magic'|\n",
    "|\\d|digit|9|\n",
    "|\\s|space|''|\n",
    "|.*|wildcard|match anything|\n",
    "|+or*|greedy match|repeats of single letter|\n",
    "|\\S|not Space|'no_spaces'|\n",
    "|[a-z]|lowercase group|'abcdefg'|\n",
    "\n",
    "### Python's re Module\n",
    "* `re` module\n",
    "* `split:` split a string on regex\n",
    "* `findall:` find all patterns in a string\n",
    "* `search:` search for a pattern\n",
    "* `match:` match an entire string or substring based on a pattern\n",
    "* Pattern first, and the string second\n",
    "* May return an iterator, string, or match object\n",
    "\n",
    "```python\n",
    "In [5]: re.split('\\s+', 'Split on spaces.')\n",
    "Out[5]: ['Split', 'on', 'spaces.']\n",
    "   \n",
    "```\n",
    "\n",
    "### Example\n",
    "\n",
    "**Practicing regular expressions: re.split() and re.findall()**\n",
    "Now you'll get a chance to write some regular expressions to match digits, strings and non-alphanumeric characters. Take a look at my_string first by printing it in the IPython Shell, to determine how you might best match the different steps.\n",
    "\n",
    "Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" followed by the character \"n\" - and not as a new line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n"
     ]
    }
   ],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "#Write a pattern called sentence_endings to match sentence endings (., ?, and !).\n",
    "sentence_endings = r\"[.!?]\"\n",
    "print(re.split(sentence_endings, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Let', 'RegEx', 'Won', 'Can', 'Or']\n"
     ]
    }
   ],
   "source": [
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_word = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_word, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n"
     ]
    }
   ],
   "source": [
    "# Split my_string on spaces and print the result\n",
    "split_word = r\"\\s+\"\n",
    "print(re.split(split_word, my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '19']\n"
     ]
    }
   ],
   "source": [
    "#find the digit in the my_string\n",
    "digit = r\"\\d+\"\n",
    "print(re.findall(digit, my_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Introduction to tokenization\n",
    "\n",
    "**What is tokenization?**\n",
    "* Turning a string or document into tokens (smaller chunks)\n",
    "* One step in preparing a text for NLP\n",
    "* Many different theories and rules\n",
    "* You can create your own rules using regular expressions\n",
    "* Some examples:\n",
    "    * Breaking out words or sentences\n",
    "    * Separating punctuation\n",
    "    * Separating all hashtags in a tweet\n",
    "  \n",
    "**nltk library**\n",
    "* nltk: natural language toolkit\n",
    "\n",
    "```python\n",
    "In [1]: from nltk.tokenize import word_tokenize\n",
    "\n",
    "In [2]: word_tokenize(\"Hi there!\") \n",
    "Out[2]: ['Hi', 'there', '!']\n",
    "```\n",
    "**Why tokenize?**\n",
    "* Easier to map part of speech\n",
    "* Matching common words\n",
    "* Removing unwanted tokens\n",
    "* \"I don't like Sam's shoes.\"\n",
    "    * \"I\", \"do\", \"n't\", \"like\", \"Sam\", \"'s\", \"shoes\", \".\"\n",
    "\n",
    "**Other nltk tokenizers**\n",
    "* `sent_tokenize:` tokenize a document into sentences\n",
    "\n",
    "* `regexp_tokenize:` tokenize a string or document based on a regular expression pattern\n",
    "\n",
    "* `TweetTokenizer:` special class just for tweet tokenization, allowing you to separate hashtags, mentions and lots of exclamation points!!!\n",
    "\n",
    "**More regex practice**\n",
    "* Difference between re.search() and re.match()\n",
    "* when use same pattern ans string on re.match() and re.search()\n",
    "\n",
    "```python\n",
    "In [1]: import re\n",
    "\n",
    "In [2]: re.match('abc', 'abcde')\n",
    "Out[2]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
    "\n",
    "In [3]: re.search('abc', 'abcde')\n",
    "Out[3]: <_sre.SRE_Match object; span=(0, 3), match='abc'>\n",
    "\n",
    "In [4]: re.match('cd', 'abcde')\n",
    "\n",
    "In [5]: re.search('cd', 'abcde')\n",
    "Out[5]: <_sre.SRE_Match object; span=(2, 4), match='cd'>\n",
    "```\n",
    "\n",
    "* re.match() find the word from begining, if the match is not in the beginning match is not able to find anyting\n",
    "* re.search() search the entrie string \n",
    "\n",
    "### Examples\n",
    "\n",
    "**Word tokenization with NLTK**\n",
    "\n",
    "Here, you'll be using the first scene of Monty Python's Holy Grail, which has been found in  scene_one. \n",
    "\n",
    "Your job in this exercise is to utilize word_tokenize and sent_tokenize from nltk.tokenize to tokenize both words and sentences from Python strings - in this case, the first scene of Monty Python's Holy Grail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_one =\"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aysbt/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ">>> import nltk\n",
    ">>> nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'not', 'We', 'halves', 'weight', 'they', 'coconut', 'then', 'velocity', 'must', 'will', 'bangin', 'at', 'carry', 'maybe', 'he', 'maintain', 'there', 'SOLDIER', \"'d\", \"'em\", '!', 'get', 'times', 'south', 'clop', 'Arthur', 'all', 'Well', 'why', '2', 'by', 'ARTHUR', 'A', '#', 'snows', 'migrate', 'in', 'my', 'Mercea', 'ridden', 'these', 'European', 'your', 'anyway', 'using', '[', 'house', 'speak', 'court', 'question', 'where', \"'\", 'five', 'air-speed', 'our', 'climes', 'coconuts', 'lord', 'wings', 'here', 'am', 'sun', 'other', 'with', 'beat', 'it', 'Not', \"n't\", 'strangers', 'son', 'of', 'African', 'forty-three', 'In', 'Britons', 'Are', 'right', 'feathers', 'But', 'have', 'carried', 'Who', 'suggesting', 'strand', 'swallows', 'length', 'What', 'Found', 'Halt', 'non-migratory', 'two', 'may', 'zone', 'husk', 'or', 'grips', ',', 'minute', 'the', 'plover', 'Ridden', 'if', 'goes', 'you', 'does', 'through', 'knights', 'King', 'Patsy', 'be', 'needs', 'I', 'covered', 'but', 'just', 'on', 'mean', '...', 'is', 'pound', 'servant', 'order', 'an', 'held', 'swallow', 'under', 'land', 'since', 'its', 'defeator', ']', 'together', 'yeah', 'You', 'guiding', '--', 'Am', 'a', 'creeper', 'Oh', 'from', 'Supposing', 'ratios', 'join', 'The', 'grip', 'Court', 'dorsal', 'It', 'SCENE', \"'m\", 'No', 'So', 'that', 'wind', 'sovereign', 'use', 'Pull', 'tell', 'Yes', 'and', 'seek', 'could', 'this', 'kingdom', 'yet', \"'s\", 'do', 'warmer', 'carrying', 'interested', ':', 'every', 'agree', 'me', 'Where', 'trusty', 'castle', 'master', 'Please', 'Uther', 'bird', 'them', 'search', '.', 'got', 'found', 'back', 'go', 'simple', 'Listen', 'second', 'line', 'temperate', '1', 'who', 'bring', 'Saxons', 'horse', 'fly', 'Will', 'martin', 'England', 'ask', 'tropical', 'They', \"'ve\", 'winter', 'course', 'wants', 'point', 'Camelot', 'Pendragon', 'That', 'Wait', \"'re\", 'empty', 'ounce', 'breadth', 'are', 'to', 'matter', 'KING', 'one', '?', 'Whoa'}\n"
     ]
    }
   ],
   "source": [
    "#Tokenize all the sentences in scene_one using the sent_tokenize() function.\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
    "tokenized_sent = word_tokenize(sentences[3])\n",
    "# Make a set of unique tokens in the entire scene: unique_tokens\n",
    "unique_tokens = set(word_tokenize(scene_one))\n",
    "print(unique_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More regex with re.search()\n",
    "In this exercise, you'll utilize re.search() and re.match() to find specific tokens. Both search and match expect regex patterns, similar to those you defined in an earlier exercise. You'll apply these regex library methods to the same Monty Python text from the nltk corpora.\n",
    "\n",
    "You have both scene_one and sentences available from the last exercise; now you can use them with re.search() and re.match() to extract and match more text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "580 588\n",
      "<re.Match object; span=(9, 32), match='[wind] [clop clop clop]'>\n",
      "<re.Match object; span=(0, 7), match='ARTHUR:'>\n"
     ]
    }
   ],
   "source": [
    "#Use re.search() to search for the first occurance of the word \"coconuts\" in scene_one. Store the result in match\n",
    "match = re.search('coconuts', scene_one)\n",
    "#Print the start and end indexes of match using its .start() and .end() methods, respectively\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: patter\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print i\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced tokenization with NLTK and regex\n",
    "**Regex groups using or \"|\"**\n",
    "* OR is represented using |\n",
    "* You can define a group using ()\n",
    "* You can define explicit character ranges using []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['He', 'has', '11', 'cats']\n"
     ]
    }
   ],
   "source": [
    "# ('(\\d+|\\w+)') finds all word or digits\n",
    "mathes_digits_and_word = ('(\\d+|\\w+)')\n",
    "print(re.findall(mathes_digits_and_word,'He has 11 cats.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regex ranges and groups\n",
    "\n",
    "|pattern|\tmatches|\texample|\n",
    "|---|---|---|\n",
    "|[A-Za-z]+|upper and lowercase English alphabet\t|'ABCDEFghijk'|\n",
    "|[0-9]|\tnumbers from 0 to 9\t|9|\n",
    "|[A-Za-z\\-\\.]+\t|upper and lowercase English alphabet, - and .\t|'My-Website.com'\n",
    "|(a-z)|\ta, - and z\t|'a-z'|\n",
    "|(\\s+l,)|\tspaces or a comma\t|', '|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 35), match='match lowercase spaces nums like 12'>\n"
     ]
    }
   ],
   "source": [
    "my_str = 'match lowercase spaces nums like 12, but no commas'\n",
    "print(re.match('[a-z0-9 ]+', my_str))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "#### Regex with NLTK tokenization\n",
    "Twitter is a frequently used source for NLP text and tasks. In this exercise, you'll build a more complex tokenizer for tweets with hashtags and mentions using nltk and regex. The nltk.tokenize.TweetTokenizer class gives you some extra methods and attributes for parsing tweets.\n",
    "\n",
    "Here, you're given some example tweets to parse using both TweetTokenizer and regexp_tokenize from the nltk.tokenize module. These example tweets have been pre-loaded into the variable tweets. Feel free to explore it in the IPython Shell!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets =['This is the best #nlp exercise ive found online! #python',\n",
    " '#NLP is super fun! <3 #learning',\n",
    " 'Thanks @datacamp :) #nlp #python']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
     ]
    }
   ],
   "source": [
    "#import TweetTokenizer\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([@dDataCamp]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-ascii tokenization\n",
    "In this exercise, you'll practice advanced tokenization by tokenizing some non-ascii based text. You'll be using German with emoji!\n",
    "\n",
    "Here, you have access to a string called german_text, which has been printed for you in the Shell. Notice the emoji and the German characters!\n",
    "\n",
    "The following modules have been pre-imported from nltk.tokenize: regexp_tokenize and word_tokenize.\n",
    "\n",
    "Unicode ranges for emoji are:\n",
    "\n",
    "```python\n",
    "('\\U0001F300'-'\\U0001F5FF'), ('\\U0001F600-\\U0001F64F'), ('\\U0001F680-\\U0001F6FF'), and ('\\u2600'-\\u26FF-\\u2700-\\u27BF')```\n",
    "                                                                         \n",
    "                                                                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_text ='Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print all words in german_text\n",
    "print(word_tokenize(german_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wann', 'Pizza', 'Und', 'Über']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only capital words\n",
    "capital_worlds = r'[A-ZÜ]\\w+'\n",
    "print(regexp_tokenize(german_text, capital_worlds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['🍕', '🚕']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and print only emoji\n",
    "#define the unicode for emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Charting word length with NLTK\n",
    "Getting started with matplotlib\n",
    "Charting library used by many open source Python projects\n",
    "Straightforward functionality with lots of options\n",
    "Histograms\n",
    "Bar charts\n",
    "Line charts\n",
    "Scatter plots\n",
    "... and also advanced functionality like 3D graphs and animations!\n",
    "#### Combining NLP data extraction with plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "words = word_tokenize(\"This is a pretty cool tool!\")\n",
    "#it creates a list of lenght from tokenized words\n",
    "words_lengths = [len(w) for w in words]\n",
    "plt.hist(words_lengths)\n",
    "plt.xlabel('lengths of words')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "#### Charting practice\n",
    "Try using your new skills to find and chart the number of words per line in the script using matplotlib. The Holy Grail script is loaded for you, and you need to use regex to find the words per line.\n",
    "\n",
    "Using list comprehensions here will speed up your computations. For example: my_lines = [tokenize(l) for l in lines] will call a function tokenize on each line in the list lines. The new transformed list will be saved in the my_lines variable.\n",
    "\n",
    "You have access to the entire script in the variable holy_grail. Go for it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FIRST LOAD THE HOLY GRAIL TEXT\n",
    "file_name = 'data/grail.txt'\n",
    "holy_grill = open(file_name, 'r')\n",
    "lines = holy_grill.read()\n",
    "split_line = lines.split('\\n')\n",
    "#print(split_line)\n",
    "#Use re.sub() inside a list comprehension to replace the prompts such as ARTHUR: and SOLDIER #1.\n",
    "# Replace all script lines for speaker\n",
    "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
    "lines_re_sub = [re.sub(pattern, '', l) for l in split_line]\n",
    "# Tokenize each line: tokenized_lines\n",
    "tokenized_lines = [regexp_tokenize(s, '\\w+') for s in lines_re_sub]\n",
    "\n",
    "# Make a frequency list of lengths: line_num_words\n",
    "num_of_word_in_line = [len(t_line) for t_line in tokenized_lines]\n",
    "\n",
    "plt.hist(num_of_word_in_line)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
